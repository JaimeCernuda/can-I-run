{
  "_comment": "Quantization specifications with quality factors. Sources: llama.cpp quantize tool (https://github.com/ggml-org/llama.cpp/blob/master/tools/quantize/README.md), Intel Low-bit Quantized Open LLM Leaderboard (https://huggingface.co/spaces/Intel/low_bit_open_llm_leaderboard)",
  "quantizations": [
    {
      "name": "F16",
      "bits_per_weight": 16.0,
      "quality_factor": 1.0,
      "ppl_increase": 0.0,
      "quality_tier": "near_lossless",
      "source": "Baseline (no quantization)"
    },
    {
      "name": "Q8_0",
      "bits_per_weight": 8.5,
      "quality_factor": 0.9995,
      "ppl_increase": 0.003,
      "quality_tier": "near_lossless",
      "source": "llama.cpp"
    },
    {
      "name": "Q6_K",
      "bits_per_weight": 6.57,
      "quality_factor": 0.9965,
      "ppl_increase": 0.022,
      "quality_tier": "very_low_loss",
      "source": "llama.cpp"
    },
    {
      "name": "Q5_K_M",
      "bits_per_weight": 5.67,
      "quality_factor": 0.9909,
      "ppl_increase": 0.057,
      "quality_tier": "recommended",
      "source": "llama.cpp"
    },
    {
      "name": "Q5_K_S",
      "bits_per_weight": 5.53,
      "quality_factor": 0.9834,
      "ppl_increase": 0.105,
      "quality_tier": "balanced",
      "source": "llama.cpp"
    },
    {
      "name": "Q4_K_M",
      "bits_per_weight": 4.83,
      "quality_factor": 0.9914,
      "ppl_increase": 0.054,
      "quality_tier": "recommended",
      "source": "llama.cpp - Sweet spot"
    },
    {
      "name": "Q4_K_S",
      "bits_per_weight": 4.58,
      "quality_factor": 0.9873,
      "ppl_increase": 0.08,
      "quality_tier": "balanced",
      "source": "llama.cpp"
    },
    {
      "name": "Q4_0",
      "bits_per_weight": 4.34,
      "quality_factor": 0.93,
      "ppl_increase": 0.469,
      "quality_tier": "noticeable_loss",
      "source": "llama.cpp (legacy)"
    },
    {
      "name": "IQ4_XS",
      "bits_per_weight": 4.25,
      "quality_factor": 0.9858,
      "ppl_increase": 0.09,
      "quality_tier": "balanced",
      "source": "llama.cpp IQ"
    },
    {
      "name": "Q3_K_M",
      "bits_per_weight": 3.89,
      "quality_factor": 0.9623,
      "ppl_increase": 0.244,
      "quality_tier": "noticeable_loss",
      "source": "llama.cpp"
    },
    {
      "name": "Q3_K_S",
      "bits_per_weight": 3.5,
      "quality_factor": 0.9046,
      "ppl_increase": 0.657,
      "quality_tier": "high_loss",
      "source": "llama.cpp"
    },
    {
      "name": "IQ3_M",
      "bits_per_weight": 3.44,
      "quality_factor": 0.9468,
      "ppl_increase": 0.35,
      "quality_tier": "noticeable_loss",
      "source": "llama.cpp IQ"
    },
    {
      "name": "IQ3_S",
      "bits_per_weight": 3.25,
      "quality_factor": 0.9326,
      "ppl_increase": 0.45,
      "quality_tier": "noticeable_loss",
      "source": "llama.cpp IQ"
    },
    {
      "name": "Q2_K",
      "bits_per_weight": 3.0,
      "quality_factor": 0.8775,
      "ppl_increase": 0.87,
      "quality_tier": "high_loss",
      "source": "llama.cpp"
    },
    {
      "name": "IQ2_M",
      "bits_per_weight": 2.7,
      "quality_factor": 0.8385,
      "ppl_increase": 1.2,
      "quality_tier": "high_loss",
      "source": "llama.cpp IQ"
    },
    {
      "name": "IQ2_S",
      "bits_per_weight": 2.5,
      "quality_factor": 0.7758,
      "ppl_increase": 1.8,
      "quality_tier": "extreme_loss",
      "source": "llama.cpp IQ"
    },
    {
      "name": "IQ2_XS",
      "bits_per_weight": 2.43,
      "quality_factor": 0.7136,
      "ppl_increase": 2.5,
      "quality_tier": "extreme_loss",
      "source": "llama.cpp IQ"
    },
    {
      "name": "IQ2_XXS",
      "bits_per_weight": 2.24,
      "quality_factor": 0.639,
      "ppl_increase": 3.52,
      "quality_tier": "extreme_loss",
      "source": "llama.cpp"
    },
    {
      "name": "IQ1_M",
      "bits_per_weight": 2.01,
      "quality_factor": 0.4378,
      "ppl_increase": 8.0,
      "quality_tier": "extreme_loss",
      "source": "llama.cpp IQ"
    },
    {
      "name": "IQ1_S",
      "bits_per_weight": 1.88,
      "quality_factor": 0.3417,
      "ppl_increase": 12.0,
      "quality_tier": "extreme_loss",
      "source": "llama.cpp IQ"
    }
  ]
}