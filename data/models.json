{
  "models": [
    {
      "name": "mistral-7b-v0.3",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 32768,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-0.6B",
      "total_params_b": 0.6,
      "active_params_b": 0.6,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1024,
      "num_layers": 28,
      "num_heads": 16,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gpt-oss-20b-BF16",
      "total_params_b": 20.0,
      "active_params_b": 2.5,
      "is_moe": true,
      "num_experts": 32,
      "num_active_experts": 4,
      "hidden_dim": 2880,
      "num_layers": 24,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 201088,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.1-8B-Instruct",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Meta-Llama-3.1-8B-Instruct",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-8B",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "llava-1.5-7b-hf",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 32,
      "vocab_size": 32064,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gpt-oss-20b",
      "total_params_b": 20.0,
      "active_params_b": 2.5,
      "is_moe": true,
      "num_experts": 32,
      "num_active_experts": 4,
      "hidden_dim": 2880,
      "num_layers": 24,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 201088,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-4B-Thinking",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Devstral-Small-2-24B-Instruct-2512",
      "total_params_b": 24.0,
      "active_params_b": 24.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 393216,
      "effective_context_length": 393216,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-27b-it",
      "total_params_b": 27.0,
      "active_params_b": 27.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5376,
      "num_layers": 62,
      "num_heads": 32,
      "num_kv_heads": 16,
      "vocab_size": 262208,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-1.7B",
      "total_params_b": 1.7,
      "active_params_b": 1.7,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 28,
      "num_heads": 16,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-4b-it",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 34,
      "num_heads": 8,
      "num_kv_heads": 4,
      "vocab_size": 262208,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-4B",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.2-1B-Instruct",
      "total_params_b": 1.0,
      "active_params_b": 1.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 16,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "llama-3-8b-Instruct",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-Coder-7B-Instruct",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3584,
      "num_layers": 28,
      "num_heads": 28,
      "num_kv_heads": 4,
      "vocab_size": 152064,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-Next-80B-A3B-Instruct",
      "total_params_b": 80.0,
      "active_params_b": 80.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 48,
      "num_heads": 16,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-R1-Distill-Qwen-1.5B",
      "total_params_b": 1.5,
      "active_params_b": 1.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1536,
      "num_layers": 28,
      "num_heads": 12,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-12b-it",
      "total_params_b": 12.0,
      "active_params_b": 12.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3840,
      "num_layers": 48,
      "num_heads": 16,
      "num_kv_heads": 8,
      "vocab_size": 262208,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gpt-oss-120b",
      "total_params_b": 120.0,
      "active_params_b": 3.75,
      "is_moe": true,
      "num_experts": 128,
      "num_active_experts": 4,
      "hidden_dim": 2880,
      "num_layers": 36,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 201088,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.2-3B-Instruct",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 28,
      "num_heads": 24,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Pixtral-12B-2409",
      "total_params_b": 12.0,
      "active_params_b": 12.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 1024000,
      "effective_context_length": 1024000,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-V3.1-BF16",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-14B",
      "total_params_b": 14.0,
      "active_params_b": 14.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.2-11B-Vision-Instruct",
      "total_params_b": 11.0,
      "active_params_b": 11.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-14B-Instruct",
      "total_params_b": 14.0,
      "active_params_b": 14.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 48,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-4B-Instruct-2507",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Nemotron-3-Nano-30B-A3B",
      "total_params_b": 30.0,
      "active_params_b": 30.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2688,
      "num_layers": 52,
      "num_heads": 32,
      "num_kv_heads": 2,
      "vocab_size": 131072,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "llama-3-8b",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-8B-Instruct",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-30B-A3B",
      "total_params_b": 30.0,
      "active_params_b": 30.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 48,
      "num_heads": 32,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-270m-it",
      "total_params_b": 0.26,
      "active_params_b": 0.26,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 640,
      "num_layers": 18,
      "num_heads": 4,
      "num_kv_heads": 1,
      "vocab_size": 262144,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-VL-7B-Instruct",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3584,
      "num_layers": 28,
      "num_heads": 28,
      "num_kv_heads": 4,
      "vocab_size": 152064,
      "max_context_length": 128000,
      "effective_context_length": 128000,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-7B-Instruct",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3584,
      "num_layers": 28,
      "num_heads": 28,
      "num_kv_heads": 4,
      "vocab_size": 152064,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "orpheus-3b-0.1-ft",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 28,
      "num_heads": 24,
      "num_kv_heads": 8,
      "vocab_size": 156940,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Meta-Llama-3.1-8B",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-3B-Instruct",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 36,
      "num_heads": 16,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "mistral-7b-instruct-v0.3",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 32768,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-V3-0324-BF16",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-R1-0528-Qwen3-8B",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-32B",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-4B-Base",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-1b-it",
      "total_params_b": 1.0,
      "active_params_b": 1.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1152,
      "num_layers": 26,
      "num_heads": 4,
      "num_kv_heads": 1,
      "vocab_size": 262144,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-Coder-30B-A3B-Instruct",
      "total_params_b": 30.0,
      "active_params_b": 30.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 48,
      "num_heads": 32,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gpt-oss-120b-BF16",
      "total_params_b": 120.0,
      "active_params_b": 3.75,
      "is_moe": true,
      "num_experts": 128,
      "num_active_experts": 4,
      "hidden_dim": 2880,
      "num_layers": 36,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 201088,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Phi-3-mini-4k-instruct",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 32,
      "vocab_size": 32064,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-12b-it-qat-int4",
      "total_params_b": 12.0,
      "active_params_b": 12.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3840,
      "num_layers": 48,
      "num_heads": 16,
      "num_kv_heads": 8,
      "vocab_size": 262208,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2-1.5B-Instruct",
      "total_params_b": 1.5,
      "active_params_b": 1.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1536,
      "num_layers": 28,
      "num_heads": 12,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "meta-Llama-3.1-8B",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Mistral-Small-24B-Instruct-2501",
      "total_params_b": 24.0,
      "active_params_b": 24.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-OCR",
      "total_params_b": 0.4,
      "active_params_b": 0.4,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1280,
      "num_layers": 12,
      "num_heads": 10,
      "num_kv_heads": 10,
      "vocab_size": 129280,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Mistral-Small-3.1-24B-Instruct-2503",
      "total_params_b": 24.0,
      "active_params_b": 24.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "phi-4",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 40,
      "num_kv_heads": 10,
      "vocab_size": 100352,
      "max_context_length": 16384,
      "effective_context_length": 16384,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.2-3B",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 28,
      "num_heads": 24,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-2-2b-it",
      "total_params_b": 2.0,
      "active_params_b": 2.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2304,
      "num_layers": 26,
      "num_heads": 8,
      "num_kv_heads": 4,
      "vocab_size": 256000,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-R1-Distill-Llama-8B",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-4B-Instruct",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Devstral-Small-2507",
      "total_params_b": 13.25,
      "active_params_b": 13.25,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-1.5B-Instruct",
      "total_params_b": 1.5,
      "active_params_b": 1.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1536,
      "num_layers": 28,
      "num_heads": 12,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-R1",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-VL-3B-Instruct",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 36,
      "num_heads": 16,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 128000,
      "effective_context_length": 128000,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Kimi-K2-Instruct",
      "total_params_b": 38.78,
      "active_params_b": 38.78,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 64,
      "num_kv_heads": 64,
      "vocab_size": 163840,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-4-Scout-17B-16E-Instruct",
      "total_params_b": 17.0,
      "active_params_b": 1.06,
      "is_moe": true,
      "num_experts": 16,
      "num_active_experts": 1,
      "hidden_dim": 5120,
      "num_layers": 48,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 202048,
      "max_context_length": 10485760,
      "effective_context_length": 10485760,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-0.5B-Instruct",
      "total_params_b": 0.5,
      "active_params_b": 0.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 896,
      "num_layers": 24,
      "num_heads": 14,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Ministral-3-3B-Instruct-2512",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 26,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Ministral-3-8B-Instruct-2512-FP8",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 34,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-2B-Instruct",
      "total_params_b": 2.0,
      "active_params_b": 2.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 28,
      "num_heads": 16,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Nemotron-3-Nano-30B-A3B-FP8",
      "total_params_b": 30.0,
      "active_params_b": 30.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2688,
      "num_layers": 52,
      "num_heads": 32,
      "num_kv_heads": 2,
      "vocab_size": 131072,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.3-70B-Instruct",
      "total_params_b": 70.0,
      "active_params_b": 70.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "medgemma-4b-it",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 34,
      "num_heads": 8,
      "num_kv_heads": 4,
      "vocab_size": 262208,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Meta-Llama-3.1-70B-Instruct",
      "total_params_b": 70.0,
      "active_params_b": 70.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.2-1B",
      "total_params_b": 1.0,
      "active_params_b": 1.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 16,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-2-9b",
      "total_params_b": 9.0,
      "active_params_b": 9.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3584,
      "num_layers": 42,
      "num_heads": 16,
      "num_kv_heads": 8,
      "vocab_size": 256000,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-235B-A22B-Thinking",
      "total_params_b": 235.0,
      "active_params_b": 235.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 94,
      "num_heads": 64,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3n-E4B-it",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 35,
      "num_heads": 8,
      "num_kv_heads": 2,
      "vocab_size": 262400,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "mistral-7b-instruct-v0.2",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 32000,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-12b-it-qat",
      "total_params_b": 12.0,
      "active_params_b": 12.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3840,
      "num_layers": 48,
      "num_heads": 16,
      "num_kv_heads": 8,
      "vocab_size": 262208,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "functiongemma-270m-it",
      "total_params_b": 0.26,
      "active_params_b": 0.26,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 640,
      "num_layers": 18,
      "num_heads": 4,
      "num_kv_heads": 1,
      "vocab_size": 262144,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-235B-A22B",
      "total_params_b": 235.0,
      "active_params_b": 235.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 94,
      "num_heads": 64,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "csm-1b",
      "total_params_b": 1.0,
      "active_params_b": 1.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 16,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 2051,
      "max_context_length": 2048,
      "effective_context_length": 2048,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2-VL-7B-Instruct",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3584,
      "num_layers": 28,
      "num_heads": 28,
      "num_kv_heads": 4,
      "vocab_size": 152064,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-32B-Instruct",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "mistral-7b",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 32000,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-4B-Thinking-2507",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-235B-A22B-Instruct",
      "total_params_b": 235.0,
      "active_params_b": 235.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 94,
      "num_heads": 64,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-2-9b-it",
      "total_params_b": 9.0,
      "active_params_b": 9.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3584,
      "num_layers": 42,
      "num_heads": 16,
      "num_kv_heads": 8,
      "vocab_size": 256000,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.1-8B",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-7B",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3584,
      "num_layers": 28,
      "num_heads": 28,
      "num_kv_heads": 4,
      "vocab_size": 152064,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-30B-A3B-Instruct",
      "total_params_b": 30.0,
      "active_params_b": 30.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 48,
      "num_heads": 32,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-32B-Instruct",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-R1-Distill-Llama-70B",
      "total_params_b": 70.0,
      "active_params_b": 70.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Mistral-Nemo-Instruct-2407",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-30B-A3B-Instruct-2507",
      "total_params_b": 30.0,
      "active_params_b": 30.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 48,
      "num_heads": 32,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2-7B",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3584,
      "num_layers": 28,
      "num_heads": 28,
      "num_kv_heads": 4,
      "vocab_size": 152064,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-1.5B",
      "total_params_b": 1.5,
      "active_params_b": 1.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1536,
      "num_layers": 28,
      "num_heads": 12,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Phi-3.5-mini-instruct",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 32,
      "vocab_size": 32064,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-Coder-1.5B-Instruct",
      "total_params_b": 1.5,
      "active_params_b": 1.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1536,
      "num_layers": 28,
      "num_heads": 12,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-4-Maverick-17B-128E-Instruct",
      "total_params_b": 17.0,
      "active_params_b": 0.13,
      "is_moe": true,
      "num_experts": 128,
      "num_active_experts": 1,
      "hidden_dim": 5120,
      "num_layers": 48,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 202048,
      "max_context_length": 1048576,
      "effective_context_length": 1048576,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "embeddinggemma-300m",
      "total_params_b": 0.37,
      "active_params_b": 0.37,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 768,
      "num_layers": 24,
      "num_heads": 3,
      "num_kv_heads": 1,
      "vocab_size": 262144,
      "max_context_length": 2048,
      "effective_context_length": 2048,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-30B-A3B-128K",
      "total_params_b": 30.0,
      "active_params_b": 30.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 48,
      "num_heads": 32,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-2b-it",
      "total_params_b": 2.0,
      "active_params_b": 2.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 18,
      "num_heads": 8,
      "num_kv_heads": 1,
      "vocab_size": 256000,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-R1-Distill-Qwen-7B",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3584,
      "num_layers": 28,
      "num_heads": 28,
      "num_kv_heads": 4,
      "vocab_size": 152064,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-Coder-32B-Instruct",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Phi-4-mini-reasoning",
      "total_params_b": 4.24,
      "active_params_b": 4.24,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 32,
      "num_heads": 24,
      "num_kv_heads": 8,
      "vocab_size": 200064,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Devstral-Small-2505",
      "total_params_b": 13.25,
      "active_params_b": 13.25,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-27b-it-qat",
      "total_params_b": 27.0,
      "active_params_b": 27.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5376,
      "num_layers": 62,
      "num_heads": 32,
      "num_kv_heads": 16,
      "vocab_size": 262208,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Phi-4-mini-instruct",
      "total_params_b": 4.24,
      "active_params_b": 4.24,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 32,
      "num_heads": 24,
      "num_kv_heads": 8,
      "vocab_size": 200064,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Ministral-3-14B-Instruct-2512-FP8",
      "total_params_b": 14.0,
      "active_params_b": 14.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-14B-128K",
      "total_params_b": 14.0,
      "active_params_b": 14.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-0.5B",
      "total_params_b": 0.5,
      "active_params_b": 0.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 896,
      "num_layers": 24,
      "num_heads": 14,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3n-E4B",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 35,
      "num_heads": 8,
      "num_kv_heads": 2,
      "vocab_size": 262400,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-Coder-7B",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3584,
      "num_layers": 28,
      "num_heads": 28,
      "num_kv_heads": 4,
      "vocab_size": 152064,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "tinyllama-chat",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 22,
      "num_heads": 32,
      "num_kv_heads": 4,
      "vocab_size": 32000,
      "max_context_length": 2048,
      "effective_context_length": 2048,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-R1-0528",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "MiMo-VL-7B-RL",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151680,
      "max_context_length": 128000,
      "effective_context_length": 128000,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-4b-pt",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 34,
      "num_heads": 8,
      "num_kv_heads": 4,
      "vocab_size": 262208,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2-VL-2B-Instruct",
      "total_params_b": 2.0,
      "active_params_b": 2.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1536,
      "num_layers": 28,
      "num_heads": 12,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-Coder-14B-Instruct",
      "total_params_b": 14.0,
      "active_params_b": 14.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 48,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Mistral-Small-Instruct-2409",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 6144,
      "num_layers": 56,
      "num_heads": 48,
      "num_kv_heads": 8,
      "vocab_size": 32768,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Cosmos-Reason1-7B",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3584,
      "num_layers": 28,
      "num_heads": 28,
      "num_kv_heads": 4,
      "vocab_size": 152064,
      "max_context_length": 128000,
      "effective_context_length": 128000,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "tinyllama",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 22,
      "num_heads": 32,
      "num_kv_heads": 4,
      "vocab_size": 32000,
      "max_context_length": 2048,
      "effective_context_length": 2048,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Ministral-3-8B-Instruct-2512",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 34,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-R1-Distill-Qwen-32B",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Mistral-Small-3.2-24B-Instruct-2506",
      "total_params_b": 24.0,
      "active_params_b": 24.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "QwQ-32B",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-Math-1.5B",
      "total_params_b": 1.5,
      "active_params_b": 1.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1536,
      "num_layers": 28,
      "num_heads": 12,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "llama-2-7b",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 32,
      "vocab_size": 32000,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "medgemma-27b-text-it",
      "total_params_b": 27.0,
      "active_params_b": 27.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5376,
      "num_layers": 62,
      "num_heads": 32,
      "num_kv_heads": 16,
      "vocab_size": 262144,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Phi-4-reasoning-plus",
      "total_params_b": 13.1,
      "active_params_b": 13.1,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 40,
      "num_kv_heads": 10,
      "vocab_size": 100352,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-Coder-3B-Instruct",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 36,
      "num_heads": 16,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-2-2b",
      "total_params_b": 2.0,
      "active_params_b": 2.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2304,
      "num_layers": 26,
      "num_heads": 8,
      "num_kv_heads": 4,
      "vocab_size": 256000,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-R1-Distill-Qwen-14B",
      "total_params_b": 14.0,
      "active_params_b": 14.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 48,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-14B-Base",
      "total_params_b": 14.0,
      "active_params_b": 14.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-4b-it-qat",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 34,
      "num_heads": 8,
      "num_kv_heads": 4,
      "vocab_size": 262208,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-8B-FP8",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3n-E2B-it",
      "total_params_b": 2.0,
      "active_params_b": 2.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 30,
      "num_heads": 8,
      "num_kv_heads": 2,
      "vocab_size": 262400,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Ministral-3-14B-Instruct-2512",
      "total_params_b": 14.0,
      "active_params_b": 14.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "llama-2-7b-chat",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 32,
      "vocab_size": 32000,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-V3-0324",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-0.6B-Base",
      "total_params_b": 0.6,
      "active_params_b": 0.6,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1024,
      "num_layers": 28,
      "num_heads": 16,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-8B-Base",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "orpheus-3b-0.1-pretrained",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 28,
      "num_heads": 24,
      "num_kv_heads": 8,
      "vocab_size": 156939,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "GLM-4-32B-0414",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 6144,
      "num_layers": 61,
      "num_heads": 48,
      "num_kv_heads": 2,
      "vocab_size": 151552,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-270m",
      "total_params_b": 0.26,
      "active_params_b": 0.26,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 640,
      "num_layers": 18,
      "num_heads": 4,
      "num_kv_heads": 1,
      "vocab_size": 262144,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-2b",
      "total_params_b": 2.0,
      "active_params_b": 2.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 18,
      "num_heads": 8,
      "num_kv_heads": 1,
      "vocab_size": 256000,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-1.7B-Base",
      "total_params_b": 1.7,
      "active_params_b": 1.7,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 28,
      "num_heads": 16,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-Coder-0.5B-Instruct",
      "total_params_b": 0.5,
      "active_params_b": 0.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 896,
      "num_layers": 24,
      "num_heads": 14,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-72B-Instruct",
      "total_params_b": 72.0,
      "active_params_b": 72.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Mistral-Small-3.1-24B-Base-2503",
      "total_params_b": 24.0,
      "active_params_b": 24.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-VL-72B-Instruct",
      "total_params_b": 72.0,
      "active_params_b": 72.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 128000,
      "effective_context_length": 128000,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Meta-Llama-3.1-70B",
      "total_params_b": 70.0,
      "active_params_b": 70.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Mistral-Nemo-Base-2407",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2-0.5B-Instruct",
      "total_params_b": 0.5,
      "active_params_b": 0.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 896,
      "num_layers": 24,
      "num_heads": 14,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-32B-128K",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-3B",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 36,
      "num_heads": 16,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-8B-128K",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-4B-128K",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "GLM-4-9B-0414",
      "total_params_b": 9.0,
      "active_params_b": 9.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 2,
      "vocab_size": 151552,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-4.0-h-1b",
      "total_params_b": 1.0,
      "active_params_b": 1.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1536,
      "num_layers": 40,
      "num_heads": 12,
      "num_kv_heads": 4,
      "vocab_size": 100352,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-30B-A3B-Thinking-2507",
      "total_params_b": 30.0,
      "active_params_b": 30.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 48,
      "num_heads": 32,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Ministral-3-3B-Reasoning-2512",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 26,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "GLM-4.6V-Flash",
      "total_params_b": 8.67,
      "active_params_b": 8.67,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 2,
      "vocab_size": 151552,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2-7B-Instruct",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3584,
      "num_layers": 28,
      "num_heads": 28,
      "num_kv_heads": 4,
      "vocab_size": 152064,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-OuteTTS-1.0-1B",
      "total_params_b": 1.0,
      "active_params_b": 1.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 16,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 134400,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "LFM2-700M",
      "total_params_b": 0.55,
      "active_params_b": 0.55,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1536,
      "num_layers": 16,
      "num_heads": 24,
      "num_kv_heads": 8,
      "vocab_size": 65536,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2-1.5B",
      "total_params_b": 1.5,
      "active_params_b": 1.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1536,
      "num_layers": 28,
      "num_heads": 12,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-Math-7B",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3584,
      "num_layers": 28,
      "num_heads": 28,
      "num_kv_heads": 4,
      "vocab_size": 152064,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-8B-Thinking",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-Prover-V2-7B",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 30,
      "num_heads": 32,
      "num_kv_heads": 32,
      "vocab_size": 102400,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-VL-32B-Instruct",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 128000,
      "effective_context_length": 128000,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-4.0-h-micro",
      "total_params_b": 2.22,
      "active_params_b": 2.22,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 100352,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.2-90B-Vision",
      "total_params_b": 90.0,
      "active_params_b": 90.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 100,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-Coder-1.5B",
      "total_params_b": 1.5,
      "active_params_b": 1.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1536,
      "num_layers": 28,
      "num_heads": 12,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-1b-pt",
      "total_params_b": 1.0,
      "active_params_b": 1.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1152,
      "num_layers": 26,
      "num_heads": 4,
      "num_kv_heads": 1,
      "vocab_size": 262144,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Seed-Coder-8B-Reasoning",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 155136,
      "max_context_length": 65536,
      "effective_context_length": 65536,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "codegemma-2b",
      "total_params_b": 2.0,
      "active_params_b": 2.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 18,
      "num_heads": 8,
      "num_kv_heads": 1,
      "vocab_size": 256000,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-2-27b-it",
      "total_params_b": 27.0,
      "active_params_b": 27.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4608,
      "num_layers": 46,
      "num_heads": 32,
      "num_kv_heads": 16,
      "vocab_size": 256000,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-4-Maverick-17B-128E-Instruct-UD",
      "total_params_b": 17.0,
      "active_params_b": 0.13,
      "is_moe": true,
      "num_experts": 128,
      "num_active_experts": 1,
      "hidden_dim": 5120,
      "num_layers": 48,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 202048,
      "max_context_length": 1048576,
      "effective_context_length": 1048576,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-Math-1.5B-Instruct",
      "total_params_b": 1.5,
      "active_params_b": 1.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1536,
      "num_layers": 28,
      "num_heads": 12,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "SmolLM3-3B",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 36,
      "num_heads": 16,
      "num_kv_heads": 4,
      "vocab_size": 128256,
      "max_context_length": 65536,
      "effective_context_length": 65536,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-4.0-micro",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 40,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 100352,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-Coder-3B",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 36,
      "num_heads": 16,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Seed-Coder-8B-Instruct",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 155136,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-Math-7B-Instruct",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3584,
      "num_layers": 28,
      "num_heads": 28,
      "num_kv_heads": 4,
      "vocab_size": 152064,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Ministral-3-14B-Reasoning-2512",
      "total_params_b": 14.0,
      "active_params_b": 14.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Magistral-Small-2509-FP8-Dynamic",
      "total_params_b": 13.25,
      "active_params_b": 13.25,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-235B-A22B-128K",
      "total_params_b": 235.0,
      "active_params_b": 235.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 94,
      "num_heads": 64,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-V3-0324-UD",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "llama-3-70b",
      "total_params_b": 70.0,
      "active_params_b": 70.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Phi-4-reasoning",
      "total_params_b": 13.1,
      "active_params_b": 13.1,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 40,
      "num_kv_heads": 10,
      "vocab_size": 100352,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "ERNIE-4.5-VL-28B-A3B-PT",
      "total_params_b": 28.0,
      "active_params_b": 28.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 28,
      "num_heads": 20,
      "num_kv_heads": 4,
      "vocab_size": 103424,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Phi-3-medium-4k-instruct",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 40,
      "num_kv_heads": 10,
      "vocab_size": 32064,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "GLM-Z1-9B-0414",
      "total_params_b": 9.0,
      "active_params_b": 9.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 2,
      "vocab_size": 151552,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-R1-0528-BF16",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "LFM2-350M",
      "total_params_b": 0.27,
      "active_params_b": 0.27,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1024,
      "num_layers": 16,
      "num_heads": 16,
      "num_kv_heads": 8,
      "vocab_size": 65536,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-4.0-350m",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1024,
      "num_layers": 28,
      "num_heads": 16,
      "num_kv_heads": 4,
      "vocab_size": 100352,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "OLMo-2-0425-1B-Instruct",
      "total_params_b": 1.0,
      "active_params_b": 1.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 16,
      "num_heads": 16,
      "num_kv_heads": 16,
      "vocab_size": 100352,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Magistral-Small-2509",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "SmolLM2-135M",
      "total_params_b": 0.15,
      "active_params_b": 0.15,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 576,
      "num_layers": 30,
      "num_heads": 9,
      "num_kv_heads": 3,
      "vocab_size": 49153,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3n-E2B",
      "total_params_b": 2.0,
      "active_params_b": 2.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 30,
      "num_heads": 8,
      "num_kv_heads": 2,
      "vocab_size": 262400,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "codellama-7b",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 32,
      "vocab_size": 32016,
      "max_context_length": 16384,
      "effective_context_length": 16384,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "LFM2-1.2B",
      "total_params_b": 1.2,
      "active_params_b": 1.2,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 16,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 65536,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2-0.5B",
      "total_params_b": 0.5,
      "active_params_b": 0.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 896,
      "num_layers": 24,
      "num_heads": 14,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-32B-Thinking",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-14B",
      "total_params_b": 14.0,
      "active_params_b": 14.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 48,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-7b-it",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 28,
      "num_heads": 16,
      "num_kv_heads": 16,
      "vocab_size": 256000,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-Coder-0.5B",
      "total_params_b": 0.5,
      "active_params_b": 0.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 896,
      "num_layers": 24,
      "num_heads": 14,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3_1-Nemotron-Ultra-253B-v1",
      "total_params_b": 253.0,
      "active_params_b": 253.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 16384,
      "num_layers": 162,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Magistral-Small-2507",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Ministral-3-3B-Base-2512",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 26,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-14B-FP8",
      "total_params_b": 14.0,
      "active_params_b": 14.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Hermes-3-Llama-3.1-8B",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.1-Nemotron-Nano-4B-v1.1",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.2-11B-Vision",
      "total_params_b": 11.0,
      "active_params_b": 11.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-3.3-2b-instruct",
      "total_params_b": 2.0,
      "active_params_b": 2.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 49159,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-2-it",
      "total_params_b": 2.25,
      "active_params_b": 2.25,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2304,
      "num_layers": 26,
      "num_heads": 8,
      "num_kv_heads": 4,
      "vocab_size": 256000,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.2-1B-Instruct-FP8-Block",
      "total_params_b": 1.0,
      "active_params_b": 1.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 16,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-4.0-h-tiny",
      "total_params_b": 65.87,
      "active_params_b": 6.18,
      "is_moe": true,
      "num_experts": 64,
      "num_active_experts": 6,
      "hidden_dim": 1536,
      "num_layers": 40,
      "num_heads": 12,
      "num_kv_heads": 4,
      "vocab_size": 100352,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-12b-pt",
      "total_params_b": 12.0,
      "active_params_b": 12.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3840,
      "num_layers": 48,
      "num_heads": 16,
      "num_kv_heads": 8,
      "vocab_size": 262208,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-4-Scout-17B-16E",
      "total_params_b": 17.0,
      "active_params_b": 1.06,
      "is_moe": true,
      "num_experts": 16,
      "num_active_experts": 1,
      "hidden_dim": 5120,
      "num_layers": 48,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 202048,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "KernelLLM",
      "total_params_b": 6.97,
      "active_params_b": 6.97,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Nanonets-OCR-s",
      "total_params_b": 2.12,
      "active_params_b": 2.12,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 36,
      "num_heads": 16,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 128000,
      "effective_context_length": 128000,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-30B-A3B-Instruct-2507-FP8",
      "total_params_b": 30.0,
      "active_params_b": 30.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 48,
      "num_heads": 32,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Hermes-4-405B",
      "total_params_b": 405.0,
      "active_params_b": 405.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 16384,
      "num_layers": 126,
      "num_heads": 128,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-7b",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 28,
      "num_heads": 16,
      "num_kv_heads": 16,
      "vocab_size": 256000,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Kimi-K2-Thinking-BF16",
      "total_params_b": 38.78,
      "active_params_b": 38.78,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 64,
      "num_kv_heads": 64,
      "vocab_size": 163840,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-4B-Instruct-2507-FP8",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "SmolLM2-135M-Instruct",
      "total_params_b": 0.15,
      "active_params_b": 0.15,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 576,
      "num_layers": 30,
      "num_heads": 9,
      "num_kv_heads": 3,
      "vocab_size": 49153,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Ministral-3-8B-Reasoning-2512",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 34,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-16B-A3B",
      "total_params_b": 16.0,
      "active_params_b": 16.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 48,
      "num_heads": 32,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-2B-Thinking",
      "total_params_b": 2.0,
      "active_params_b": 2.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 28,
      "num_heads": 16,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-4B-FP8",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.1-Nemotron-Nano-8B-v1",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Ministral-3-14B-Base-2512",
      "total_params_b": 14.0,
      "active_params_b": 14.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-4b-it-FP8-Dynamic",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 34,
      "num_heads": 8,
      "num_kv_heads": 4,
      "vocab_size": 262208,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-4.0-h-350m",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 768,
      "num_layers": 32,
      "num_heads": 12,
      "num_kv_heads": 4,
      "vocab_size": 100352,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "llama-3-70b-Instruct",
      "total_params_b": 70.0,
      "active_params_b": 70.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-Prover-V2-671B",
      "total_params_b": 671.0,
      "active_params_b": 671.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-2-27b",
      "total_params_b": 27.0,
      "active_params_b": 27.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4608,
      "num_layers": 46,
      "num_heads": 32,
      "num_kv_heads": 16,
      "vocab_size": 256000,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "mistral-7b-instruct-v0.1",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 32000,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Devstral-2-123B-Instruct-2512",
      "total_params_b": 123.0,
      "active_params_b": 123.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 12288,
      "num_layers": 88,
      "num_heads": 96,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-270m-it-qat",
      "total_params_b": 0.26,
      "active_params_b": 0.26,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 640,
      "num_layers": 18,
      "num_heads": 4,
      "num_kv_heads": 1,
      "vocab_size": 262144,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3_3-Nemotron-Super-49B-v1",
      "total_params_b": 49.0,
      "active_params_b": 49.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 64,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Kimi-K2-Instruct-BF16",
      "total_params_b": 38.78,
      "active_params_b": 38.78,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 64,
      "num_kv_heads": 64,
      "vocab_size": 163840,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-4b-it-qat-int4",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 34,
      "num_heads": 8,
      "num_kv_heads": 4,
      "vocab_size": 262208,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Falcon-H1-0.5B-Instruct",
      "total_params_b": 0.5,
      "active_params_b": 0.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1024,
      "num_layers": 36,
      "num_heads": 8,
      "num_kv_heads": 2,
      "vocab_size": 32784,
      "max_context_length": 16384,
      "effective_context_length": 16384,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "zephyr-sft",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 32000,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "codegemma-7b-it",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 28,
      "num_heads": 16,
      "num_kv_heads": 16,
      "vocab_size": 256000,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "codegemma-7b",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 28,
      "num_heads": 16,
      "num_kv_heads": 16,
      "vocab_size": 256000,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-Coder-14B",
      "total_params_b": 14.0,
      "active_params_b": 14.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 48,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-32B",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-Next-80B-A3B-Thinking",
      "total_params_b": 80.0,
      "active_params_b": 80.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 48,
      "num_heads": 16,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "SmolLM2-360M-Instruct",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 960,
      "num_layers": 32,
      "num_heads": 15,
      "num_kv_heads": 5,
      "vocab_size": 49153,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-27b-pt",
      "total_params_b": 27.0,
      "active_params_b": 27.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5376,
      "num_layers": 62,
      "num_heads": 32,
      "num_kv_heads": 16,
      "vocab_size": 262208,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "llama-2-13b",
      "total_params_b": 13.0,
      "active_params_b": 13.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 40,
      "num_kv_heads": 40,
      "vocab_size": 32000,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-30B-A3B-Base",
      "total_params_b": 30.0,
      "active_params_b": 30.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 48,
      "num_heads": 32,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Ministral-3-8B-Base-2512",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 34,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "OpenHermes-2.5-Mistral-7B",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 32002,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "SmolLM2-1.7B-Instruct",
      "total_params_b": 1.7,
      "active_params_b": 1.7,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 24,
      "num_heads": 32,
      "num_kv_heads": 32,
      "vocab_size": 49153,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-1b-it-qat",
      "total_params_b": 1.0,
      "active_params_b": 1.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1152,
      "num_layers": 26,
      "num_heads": 4,
      "num_kv_heads": 1,
      "vocab_size": 262144,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "SmolLM-135M",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 576,
      "num_layers": 30,
      "num_heads": 9,
      "num_kv_heads": 3,
      "vocab_size": 49152,
      "max_context_length": 2048,
      "effective_context_length": 2048,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "AceReason-Nemotron-14B",
      "total_params_b": 14.0,
      "active_params_b": 14.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 48,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "phi-2",
      "total_params_b": 2.65,
      "active_params_b": 2.65,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 32,
      "vocab_size": 51200,
      "max_context_length": 2048,
      "effective_context_length": 2048,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-8B-Instruct-FP8",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-1b-it-FP8-Dynamic",
      "total_params_b": 1.0,
      "active_params_b": 1.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1152,
      "num_layers": 26,
      "num_heads": 4,
      "num_kv_heads": 1,
      "vocab_size": 262144,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Mistral-Large-Instruct-2407",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 12288,
      "num_layers": 88,
      "num_heads": 96,
      "num_kv_heads": 8,
      "vocab_size": 32768,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-V3-bf16",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "PaddleOCR-VL",
      "total_params_b": 0.33,
      "active_params_b": 0.33,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1024,
      "num_layers": 18,
      "num_heads": 16,
      "num_kv_heads": 2,
      "vocab_size": 103424,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-4.0-micro-base",
      "total_params_b": 3.4,
      "active_params_b": 3.4,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 40,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 100352,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-R1-UD",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Ministral-3-3B-Instruct-2512-FP8",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 26,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.2-90B-Vision-Instruct",
      "total_params_b": 90.0,
      "active_params_b": 90.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 100,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-14B-Instruct-1M",
      "total_params_b": 14.0,
      "active_params_b": 14.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 48,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 1010000,
      "effective_context_length": 1010000,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Hermes-4-70B",
      "total_params_b": 70.0,
      "active_params_b": 70.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "MAI-DS-R1",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-1.7B-FP8",
      "total_params_b": 1.7,
      "active_params_b": 1.7,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 28,
      "num_heads": 16,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "SmolLM-135M-Instruct",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 576,
      "num_layers": 30,
      "num_heads": 9,
      "num_kv_heads": 3,
      "vocab_size": 49152,
      "max_context_length": 2048,
      "effective_context_length": 2048,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "mistral-7b-v0.2",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 32000,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llasa-1B",
      "total_params_b": 1.0,
      "active_params_b": 1.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 16,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 193800,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-32B-FP8",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "r1-1776",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "r1-1776-distill-llama-70b",
      "total_params_b": 70.0,
      "active_params_b": 70.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Mixtral-8x7B-Instruct-v0.1",
      "total_params_b": 7.0,
      "active_params_b": 1.75,
      "is_moe": true,
      "num_experts": 8,
      "num_active_experts": 2,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 32000,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Olmo-3-7B-Instruct",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 32,
      "vocab_size": 100278,
      "max_context_length": 65536,
      "effective_context_length": 65536,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Olmo-3-7B-Think",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 32,
      "vocab_size": 100278,
      "max_context_length": 65536,
      "effective_context_length": 65536,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Nemotron-3-Nano-30B-A3B-Base",
      "total_params_b": 30.0,
      "active_params_b": 30.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2688,
      "num_layers": 52,
      "num_heads": 32,
      "num_kv_heads": 2,
      "vocab_size": 131072,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "INTELLECT-2",
      "total_params_b": 20.91,
      "active_params_b": 20.91,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "LFM2-2.6B-Exp",
      "total_params_b": 2.6,
      "active_params_b": 2.6,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 30,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 65536,
      "max_context_length": 128000,
      "effective_context_length": 128000,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Magistral-Small-2506",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.1-Storm-8B",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-Coder-32B",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gpt-oss-safeguard-20b-BF16",
      "total_params_b": 20.0,
      "active_params_b": 2.5,
      "is_moe": true,
      "num_experts": 32,
      "num_active_experts": 4,
      "hidden_dim": 2880,
      "num_layers": 24,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 201088,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "phi-4-reasoning",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 40,
      "num_kv_heads": 10,
      "vocab_size": 100352,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-4.0-h-small",
      "total_params_b": 4.0,
      "active_params_b": 0.56,
      "is_moe": true,
      "num_experts": 72,
      "num_active_experts": 10,
      "hidden_dim": 4096,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 100352,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-4.0-1b",
      "total_params_b": 1.0,
      "active_params_b": 1.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 40,
      "num_heads": 16,
      "num_kv_heads": 4,
      "vocab_size": 100352,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-0.6B-FP8",
      "total_params_b": 0.6,
      "active_params_b": 0.6,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1024,
      "num_layers": 28,
      "num_heads": 16,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gpt-oss-safeguard-20b",
      "total_params_b": 20.0,
      "active_params_b": 2.5,
      "is_moe": true,
      "num_experts": 32,
      "num_active_experts": 4,
      "hidden_dim": 2880,
      "num_layers": 24,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 201088,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "c4ai-command-a-03-2025",
      "total_params_b": 119.11,
      "active_params_b": 119.11,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 12288,
      "num_layers": 64,
      "num_heads": 96,
      "num_kv_heads": 8,
      "vocab_size": 256000,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Falcon-H1-7B-Instruct",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 44,
      "num_heads": 12,
      "num_kv_heads": 2,
      "vocab_size": 130049,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "NVIDIA-Nemotron-Nano-9B-v2",
      "total_params_b": 9.0,
      "active_params_b": 9.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4480,
      "num_layers": 56,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-4.0-h-tiny-FP8-Dynamic",
      "total_params_b": 65.87,
      "active_params_b": 6.18,
      "is_moe": true,
      "num_experts": 64,
      "num_active_experts": 6,
      "hidden_dim": 1536,
      "num_layers": 40,
      "num_heads": 12,
      "num_kv_heads": 4,
      "vocab_size": 100352,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-27b-it-FP8-Dynamic",
      "total_params_b": 27.0,
      "active_params_b": 27.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5376,
      "num_layers": 62,
      "num_heads": 32,
      "num_kv_heads": 16,
      "vocab_size": 262208,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-8B-Thinking-FP8",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-4B-Instruct-FP8",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-3.3-8b-instruct",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 49159,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.2-1B-Instruct-FP8-Dynamic",
      "total_params_b": 1.0,
      "active_params_b": 1.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 16,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-1.1-2b-it",
      "total_params_b": 2.0,
      "active_params_b": 2.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 18,
      "num_heads": 8,
      "num_kv_heads": 1,
      "vocab_size": 256000,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-32B-Instruct-FP8",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "QwQ-32B-Preview",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-270m-it-FP8-Dynamic",
      "total_params_b": 0.26,
      "active_params_b": 0.26,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 640,
      "num_layers": 18,
      "num_heads": 4,
      "num_kv_heads": 1,
      "vocab_size": 262144,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "SmolLM3-3B-Base",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 36,
      "num_heads": 16,
      "num_kv_heads": 4,
      "vocab_size": 128256,
      "max_context_length": 65536,
      "effective_context_length": 65536,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Seed-OSS-36B-Instruct",
      "total_params_b": 36.0,
      "active_params_b": 36.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 80,
      "num_kv_heads": 8,
      "vocab_size": 155136,
      "max_context_length": 524288,
      "effective_context_length": 524288,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "OLMo-2-0325-32B-Instruct",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 100352,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-4.0-h-tiny-base",
      "total_params_b": 65.87,
      "active_params_b": 6.18,
      "is_moe": true,
      "num_experts": 64,
      "num_active_experts": 6,
      "hidden_dim": 1536,
      "num_layers": 40,
      "num_heads": 12,
      "num_kv_heads": 4,
      "vocab_size": 100352,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-4.0-h-small-FP8-Dynamic",
      "total_params_b": 487.53,
      "active_params_b": 67.71,
      "is_moe": true,
      "num_experts": 72,
      "num_active_experts": 10,
      "hidden_dim": 4096,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 100352,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Jan-nano-128k",
      "total_params_b": 3.22,
      "active_params_b": 3.22,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "ERNIE-4.5-21B-A3B-PT",
      "total_params_b": 21.0,
      "active_params_b": 21.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 28,
      "num_heads": 20,
      "num_kv_heads": 4,
      "vocab_size": 103424,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2-VL-72B-Instruct",
      "total_params_b": 72.0,
      "active_params_b": 72.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Mixtral-8x7B-v0.1",
      "total_params_b": 7.0,
      "active_params_b": 1.75,
      "is_moe": true,
      "num_experts": 8,
      "num_active_experts": 2,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 32000,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Mistral-Small-24B-Base-2501",
      "total_params_b": 24.0,
      "active_params_b": 24.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-4.0-1b-base",
      "total_params_b": 1.0,
      "active_params_b": 1.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 40,
      "num_heads": 16,
      "num_kv_heads": 4,
      "vocab_size": 100352,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Yi-1.5-6B",
      "total_params_b": 6.0,
      "active_params_b": 6.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 4,
      "vocab_size": 64000,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2-72B-Instruct",
      "total_params_b": 72.0,
      "active_params_b": 72.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-V3.1-Terminus-BF16",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2-72B",
      "total_params_b": 72.0,
      "active_params_b": 72.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3_3-Nemotron-Super-49B-v1_5",
      "total_params_b": 49.0,
      "active_params_b": 49.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 64,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Olmo-3.1-32B-Instruct",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 100278,
      "max_context_length": 65536,
      "effective_context_length": 65536,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Olmo-3-32B-Think",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 100278,
      "max_context_length": 65536,
      "effective_context_length": 65536,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-235B-A22B-Thinking-2507",
      "total_params_b": 235.0,
      "active_params_b": 235.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 94,
      "num_heads": 64,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-12b-it-FP8-Dynamic",
      "total_params_b": 12.0,
      "active_params_b": 12.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3840,
      "num_layers": 48,
      "num_heads": 16,
      "num_kv_heads": 8,
      "vocab_size": 262208,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "codellama-34b",
      "total_params_b": 34.0,
      "active_params_b": 34.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 48,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 32000,
      "max_context_length": 16384,
      "effective_context_length": 16384,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-1.1-7b-it",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 28,
      "num_heads": 16,
      "num_kv_heads": 16,
      "vocab_size": 256000,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Apriel-1.5-15b-Thinker",
      "total_params_b": 15.0,
      "active_params_b": 15.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 48,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 262400,
      "effective_context_length": 262400,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "GLM-Z1-32B-0414",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 6144,
      "num_layers": 61,
      "num_heads": 48,
      "num_kv_heads": 2,
      "vocab_size": 151552,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "medgemma-4b-pt",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 34,
      "num_heads": 8,
      "num_kv_heads": 4,
      "vocab_size": 262208,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "SmolLM2-360M",
      "total_params_b": 0.4,
      "active_params_b": 0.4,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 960,
      "num_layers": 32,
      "num_heads": 15,
      "num_kv_heads": 5,
      "vocab_size": 49153,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "SmolLM-1.7B-Instruct",
      "total_params_b": 1.7,
      "active_params_b": 1.7,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 24,
      "num_heads": 32,
      "num_kv_heads": 32,
      "vocab_size": 49152,
      "max_context_length": 2048,
      "effective_context_length": 2048,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-4-Maverick-17B-128E",
      "total_params_b": 17.0,
      "active_params_b": 0.13,
      "is_moe": true,
      "num_experts": 128,
      "num_active_experts": 1,
      "hidden_dim": 5120,
      "num_layers": 48,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 202048,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.1-Nemotron-70B-Instruct",
      "total_params_b": 70.0,
      "active_params_b": 70.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-3.2-2b-instruct",
      "total_params_b": 2.0,
      "active_params_b": 2.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 49155,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Meta-Llama-3.1-405B",
      "total_params_b": 405.0,
      "active_params_b": 405.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 16384,
      "num_layers": 126,
      "num_heads": 128,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.1-8B-Instruct-FP8-Dynamic",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-30B-A3B-Thinking",
      "total_params_b": 30.0,
      "active_params_b": 30.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 48,
      "num_heads": 32,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-72B",
      "total_params_b": 72.0,
      "active_params_b": 72.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "medgemma-27b-it",
      "total_params_b": 27.0,
      "active_params_b": 27.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5376,
      "num_layers": 62,
      "num_heads": 32,
      "num_kv_heads": 16,
      "vocab_size": 262208,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2-VL-7B",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3584,
      "num_layers": 28,
      "num_heads": 28,
      "num_kv_heads": 4,
      "vocab_size": 152064,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Kimi-K2-Thinking",
      "total_params_b": 38.78,
      "active_params_b": 38.78,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 64,
      "num_kv_heads": 64,
      "vocab_size": 163840,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-V3.2",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-4.0-h-small-base",
      "total_params_b": 487.53,
      "active_params_b": 67.71,
      "is_moe": true,
      "num_experts": 72,
      "num_active_experts": 10,
      "hidden_dim": 4096,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 100352,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-R1-BF16",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-30B-A3B-Instruct-FP8",
      "total_params_b": 30.0,
      "active_params_b": 30.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 48,
      "num_heads": 32,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "SmolLM-360M-Instruct",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 960,
      "num_layers": 32,
      "num_heads": 15,
      "num_kv_heads": 5,
      "vocab_size": 49152,
      "max_context_length": 2048,
      "effective_context_length": 2048,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "EXAONE-4.0-32B",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 102400,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-2B-Instruct-FP8",
      "total_params_b": 2.0,
      "active_params_b": 2.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 28,
      "num_heads": 16,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "SmolLM2-1.7B",
      "total_params_b": 1.7,
      "active_params_b": 1.7,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 24,
      "num_heads": 32,
      "num_kv_heads": 32,
      "vocab_size": 49153,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "GLM-4.7",
      "total_params_b": 29.72,
      "active_params_b": 29.72,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 92,
      "num_heads": 96,
      "num_kv_heads": 8,
      "vocab_size": 151552,
      "max_context_length": 202752,
      "effective_context_length": 202752,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "LFM2-8B-A1B",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 24,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 65536,
      "max_context_length": 128000,
      "effective_context_length": 128000,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-V3",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llasa-3B",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 28,
      "num_heads": 24,
      "num_kv_heads": 8,
      "vocab_size": 193800,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "ERNIE-4.5-0.3B-PT",
      "total_params_b": 0.3,
      "active_params_b": 0.3,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1024,
      "num_layers": 18,
      "num_heads": 16,
      "num_kv_heads": 2,
      "vocab_size": 103424,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "GLM-4.5-Air",
      "total_params_b": 9.88,
      "active_params_b": 9.88,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 46,
      "num_heads": 96,
      "num_kv_heads": 8,
      "vocab_size": 151552,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "MiniMax-M2.1",
      "total_params_b": 1563.82,
      "active_params_b": 48.87,
      "is_moe": true,
      "num_experts": 256,
      "num_active_experts": 8,
      "hidden_dim": 3072,
      "num_layers": 62,
      "num_heads": 48,
      "num_kv_heads": 8,
      "vocab_size": 200064,
      "max_context_length": 196608,
      "effective_context_length": 196608,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "yi-6b",
      "total_params_b": 6.0,
      "active_params_b": 6.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 4,
      "vocab_size": 64000,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "yi-34b-chat",
      "total_params_b": 34.0,
      "active_params_b": 34.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 60,
      "num_heads": 56,
      "num_kv_heads": 8,
      "vocab_size": 64000,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.2-3B-Instruct-FP8-Dynamic",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 28,
      "num_heads": 24,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-235B-A22B-Instruct-2507",
      "total_params_b": 235.0,
      "active_params_b": 235.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 94,
      "num_heads": 64,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-R1-Zero",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-4.0-h-350m-base",
      "total_params_b": 0.3,
      "active_params_b": 0.3,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 768,
      "num_layers": 32,
      "num_heads": 12,
      "num_kv_heads": 4,
      "vocab_size": 100352,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "JanusCoder-8B",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "c4ai-command-r-08-2024",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 40,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 256000,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Meta-Llama-3.1-405B-Instruct",
      "total_params_b": 405.0,
      "active_params_b": 405.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 16384,
      "num_layers": 126,
      "num_heads": 128,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-4.0-h-micro-base",
      "total_params_b": 2.22,
      "active_params_b": 2.22,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 100352,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-235B-A22B-Thinking-FP8",
      "total_params_b": 235.0,
      "active_params_b": 235.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 94,
      "num_heads": 64,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Falcon-H1-3B-Instruct",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 32,
      "num_heads": 10,
      "num_kv_heads": 2,
      "vocab_size": 65537,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-4B-Thinking-2507-FP8",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "reka-flash-3",
      "total_params_b": 20.55,
      "active_params_b": 20.55,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 6144,
      "num_layers": 44,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 100352,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-4-Scout-17B-16E-bnb-8bit",
      "total_params_b": 17.0,
      "active_params_b": 1.06,
      "is_moe": true,
      "num_experts": 16,
      "num_active_experts": 1,
      "hidden_dim": 5120,
      "num_layers": 48,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 202048,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Kimi-K2-Base-BF16",
      "total_params_b": 38.78,
      "active_params_b": 38.78,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 64,
      "num_kv_heads": 64,
      "vocab_size": 163840,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "GLM-4.6",
      "total_params_b": 29.72,
      "active_params_b": 29.72,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 92,
      "num_heads": 96,
      "num_kv_heads": 8,
      "vocab_size": 151552,
      "max_context_length": 202752,
      "effective_context_length": 202752,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "codellama-13b",
      "total_params_b": 13.0,
      "active_params_b": 13.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 40,
      "num_kv_heads": 40,
      "vocab_size": 32016,
      "max_context_length": 16384,
      "effective_context_length": 16384,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-vision-3.2-2b",
      "total_params_b": 2.0,
      "active_params_b": 2.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 49156,
      "max_context_length": 16384,
      "effective_context_length": 16384,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-4-Maverick-17B-128E-Instruct-FP8",
      "total_params_b": 17.0,
      "active_params_b": 0.13,
      "is_moe": true,
      "num_experts": 128,
      "num_active_experts": 1,
      "hidden_dim": 5120,
      "num_layers": 48,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 202048,
      "max_context_length": 1048576,
      "effective_context_length": 1048576,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "SmolLM-360M",
      "total_params_b": 0.4,
      "active_params_b": 0.4,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 960,
      "num_layers": 32,
      "num_heads": 15,
      "num_kv_heads": 5,
      "vocab_size": 49152,
      "max_context_length": 2048,
      "effective_context_length": 2048,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2-VL-2B",
      "total_params_b": 2.0,
      "active_params_b": 2.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1536,
      "num_layers": 28,
      "num_heads": 12,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "GLM-4.6V-FP8",
      "total_params_b": 9.88,
      "active_params_b": 9.88,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 46,
      "num_heads": 96,
      "num_kv_heads": 8,
      "vocab_size": 151552,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "ERNIE-4.5-21B-A3B-Thinking",
      "total_params_b": 21.0,
      "active_params_b": 21.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 28,
      "num_heads": 20,
      "num_kv_heads": 4,
      "vocab_size": 103424,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "GLM-4.6V",
      "total_params_b": 9.88,
      "active_params_b": 9.88,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 46,
      "num_heads": 96,
      "num_kv_heads": 8,
      "vocab_size": 151552,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-Coder-480B-A35B-Instruct-1M",
      "total_params_b": 480.0,
      "active_params_b": 480.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 6144,
      "num_layers": 62,
      "num_heads": 96,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 1048576,
      "effective_context_length": 1048576,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-Math-72B-Instruct",
      "total_params_b": 72.0,
      "active_params_b": 72.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-30B-A3B-Thinking-2507-FP8",
      "total_params_b": 30.0,
      "active_params_b": 30.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 48,
      "num_heads": 32,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepScaleR-1.5B-Preview",
      "total_params_b": 1.5,
      "active_params_b": 1.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1536,
      "num_layers": 28,
      "num_heads": 12,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 24576,
      "effective_context_length": 24576,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Kimi-K2-Instruct-0905-BF16",
      "total_params_b": 38.78,
      "active_params_b": 38.78,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 64,
      "num_kv_heads": 64,
      "vocab_size": 163840,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-4-Scout-17B-16E-Instruct-bnb-8bit",
      "total_params_b": 17.0,
      "active_params_b": 1.06,
      "is_moe": true,
      "num_experts": 16,
      "num_active_experts": 1,
      "hidden_dim": 5120,
      "num_layers": 48,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 202048,
      "max_context_length": 10485760,
      "effective_context_length": 10485760,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Olmo-3.1-32B-Think",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 100278,
      "max_context_length": 65536,
      "effective_context_length": 65536,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-V3.2-Speciale",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Falcon-H1-1.5B-Instruct",
      "total_params_b": 1.5,
      "active_params_b": 1.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 24,
      "num_heads": 8,
      "num_kv_heads": 2,
      "vocab_size": 65537,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Kimi-K2-Instruct-0905",
      "total_params_b": 38.78,
      "active_params_b": 38.78,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 64,
      "num_kv_heads": 64,
      "vocab_size": 163840,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-R1-Zero-BF16",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-V3.1",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "SmolLM-1.7B",
      "total_params_b": 1.7,
      "active_params_b": 1.7,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 24,
      "num_heads": 32,
      "num_kv_heads": 32,
      "vocab_size": 49152,
      "max_context_length": 2048,
      "effective_context_length": 2048,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-235B-A22B-Instruct-2507-FP8",
      "total_params_b": 235.0,
      "active_params_b": 235.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 94,
      "num_heads": 64,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-Coder-480B-A35B-Instruct-FP8",
      "total_params_b": 480.0,
      "active_params_b": 480.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 6144,
      "num_layers": 62,
      "num_heads": 96,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "MiniMax-M2",
      "total_params_b": 1563.82,
      "active_params_b": 48.87,
      "is_moe": true,
      "num_experts": 256,
      "num_active_experts": 8,
      "hidden_dim": 3072,
      "num_layers": 62,
      "num_heads": 48,
      "num_kv_heads": 8,
      "vocab_size": 200064,
      "max_context_length": 196608,
      "effective_context_length": 196608,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "embeddinggemma-300m-qat-q8_0-unquantized",
      "total_params_b": 0.37,
      "active_params_b": 0.37,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 768,
      "num_layers": 24,
      "num_heads": 3,
      "num_kv_heads": 1,
      "vocab_size": 262144,
      "max_context_length": 2048,
      "effective_context_length": 2048,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Apertus-70B-Instruct-2509",
      "total_params_b": 70.0,
      "active_params_b": 70.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 65536,
      "effective_context_length": 65536,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.2-3B-Instruct-FP8-Block",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 28,
      "num_heads": 24,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-4.0-h-1b-base",
      "total_params_b": 1.0,
      "active_params_b": 1.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1536,
      "num_layers": 40,
      "num_heads": 12,
      "num_kv_heads": 4,
      "vocab_size": 100352,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "GLM-4.7-FP8",
      "total_params_b": 29.72,
      "active_params_b": 29.72,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 92,
      "num_heads": 96,
      "num_kv_heads": 8,
      "vocab_size": 151552,
      "max_context_length": 202752,
      "effective_context_length": 202752,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2-Math-1.5B",
      "total_params_b": 1.5,
      "active_params_b": 1.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1536,
      "num_layers": 28,
      "num_heads": 12,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Kimi-K2-Base",
      "total_params_b": 38.78,
      "active_params_b": 38.78,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 64,
      "num_kv_heads": 64,
      "vocab_size": 163840,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "embeddinggemma-300m-qat-q4_0-unquantized",
      "total_params_b": 0.37,
      "active_params_b": 0.37,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 768,
      "num_layers": 24,
      "num_heads": 3,
      "num_kv_heads": 1,
      "vocab_size": 262144,
      "max_context_length": 2048,
      "effective_context_length": 2048,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-V3.1-Terminus",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Hermes-2-Pro-Mistral-7B",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 32032,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.3-70B-Instruct-FP8-Dynamic",
      "total_params_b": 70.0,
      "active_params_b": 70.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-30B-A3B-FP8",
      "total_params_b": 30.0,
      "active_params_b": 30.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 48,
      "num_heads": 32,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Apertus-8B-Instruct-2509",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 65536,
      "effective_context_length": 65536,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Phi-3-mini-4k-instruct-v0",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 32,
      "vocab_size": 32064,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Reflection-Llama-3.1-70B",
      "total_params_b": 70.0,
      "active_params_b": 70.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128262,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-4-Scout-17B-16E-Instruct-dynamic",
      "total_params_b": 17.0,
      "active_params_b": 1.06,
      "is_moe": true,
      "num_experts": 16,
      "num_active_experts": 1,
      "hidden_dim": 5120,
      "num_layers": 48,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 202048,
      "max_context_length": 10485760,
      "effective_context_length": 10485760,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-Guard-4-12B",
      "total_params_b": 12.0,
      "active_params_b": 12.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 48,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 202048,
      "max_context_length": 10485760,
      "effective_context_length": 10485760,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-TNG-R1T2-Chimera",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3n-E4B-it-litert-preview",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 35,
      "num_heads": 8,
      "num_kv_heads": 2,
      "vocab_size": 262400,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-235B-A22B-Thinking-2507-FP8",
      "total_params_b": 235.0,
      "active_params_b": 235.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 94,
      "num_heads": 64,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Hermes-3-Llama-3.1-70B",
      "total_params_b": 70.0,
      "active_params_b": 70.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "aya-vision-8b",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 256000,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "GLM-4.5",
      "total_params_b": 29.72,
      "active_params_b": 29.72,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 92,
      "num_heads": 96,
      "num_kv_heads": 8,
      "vocab_size": 151552,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3n-E2B-it-litert-preview",
      "total_params_b": 2.0,
      "active_params_b": 2.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 30,
      "num_heads": 8,
      "num_kv_heads": 2,
      "vocab_size": 262400,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-V3.1-Base-BF16",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-Math-V2",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "SmolLM3-3B-128K",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 36,
      "num_heads": 16,
      "num_kv_heads": 4,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-Coder-480B-A35B-Instruct",
      "total_params_b": 480.0,
      "active_params_b": 480.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 6144,
      "num_layers": 62,
      "num_heads": 96,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "grok-2",
      "total_params_b": 336.73,
      "active_params_b": 84.18,
      "is_moe": true,
      "num_experts": 8,
      "num_active_experts": 2,
      "hidden_dim": 8192,
      "num_layers": 64,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-32B-Thinking-FP8",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.3-Nemotron-70B-Edit",
      "total_params_b": 70.0,
      "active_params_b": 70.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Hunyuan-A13B-Instruct",
      "total_params_b": 13.0,
      "active_params_b": 13.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128167,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Pixtral-12B-Base-2409",
      "total_params_b": 12.0,
      "active_params_b": 12.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 1024000,
      "effective_context_length": 1024000,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-3.2-8b-instruct",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 49155,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Magistral-Small-2509-FP8-torchao",
      "total_params_b": 13.25,
      "active_params_b": 13.25,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 131072,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-7B-Instruct-1M",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3584,
      "num_layers": 28,
      "num_heads": 28,
      "num_kv_heads": 4,
      "vocab_size": 152064,
      "max_context_length": 1010000,
      "effective_context_length": 1010000,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-30B-A3B-Thinking-FP8",
      "total_params_b": 30.0,
      "active_params_b": 30.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 48,
      "num_heads": 32,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "c4ai-command-r-plus-08-2024",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 12288,
      "num_layers": 64,
      "num_heads": 96,
      "num_kv_heads": 8,
      "vocab_size": 256000,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "QVQ-72B-Preview",
      "total_params_b": 72.0,
      "active_params_b": 72.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 128000,
      "effective_context_length": 128000,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gemma-3-270m-it-torchao-FP8",
      "total_params_b": 0.26,
      "active_params_b": 0.26,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 640,
      "num_layers": 18,
      "num_heads": 4,
      "num_kv_heads": 1,
      "vocab_size": 262144,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-V3.1-Base",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-4B-Thinking-FP8",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "granite-4.0-350m-base",
      "total_params_b": 0.46,
      "active_params_b": 0.46,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1024,
      "num_layers": 28,
      "num_heads": 16,
      "num_kv_heads": 4,
      "vocab_size": 100352,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-V3.2-Exp",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2-VL-72B",
      "total_params_b": 72.0,
      "active_params_b": 72.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "aya-vision-32b",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 40,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 256000,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "DeepSeek-TNG-R1T2-Chimera-BF16",
      "total_params_b": 38.54,
      "active_params_b": 38.54,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 129280,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.3-70B-Instruct-FP8-Block",
      "total_params_b": 70.0,
      "active_params_b": 70.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.1-70B",
      "total_params_b": 70.0,
      "active_params_b": 70.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.2-3B-FP8-Dynamic",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 28,
      "num_heads": 24,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "yi-34b",
      "total_params_b": 34.0,
      "active_params_b": 34.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 60,
      "num_heads": 56,
      "num_kv_heads": 8,
      "vocab_size": 64000,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-Math-72B",
      "total_params_b": 72.0,
      "active_params_b": 72.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 4096,
      "effective_context_length": 4096,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-4-Scout-17B-16E-dynamic",
      "total_params_b": 17.0,
      "active_params_b": 1.06,
      "is_moe": true,
      "num_experts": 16,
      "num_active_experts": 1,
      "hidden_dim": 5120,
      "num_layers": 48,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 202048,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Falcon-H1-34B-Instruct",
      "total_params_b": 34.0,
      "active_params_b": 34.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 72,
      "num_heads": 20,
      "num_kv_heads": 4,
      "vocab_size": 261120,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "ERNIE-4.5-300B-A47B-PT",
      "total_params_b": 300.0,
      "active_params_b": 300.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 54,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 103424,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "GLM-4.1V-9B-Thinking",
      "total_params_b": 9.0,
      "active_params_b": 9.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 40,
      "num_heads": 32,
      "num_kv_heads": 2,
      "vocab_size": 151552,
      "max_context_length": 65536,
      "effective_context_length": 65536,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gpt-oss-safeguard-120b",
      "total_params_b": 120.0,
      "active_params_b": 3.75,
      "is_moe": true,
      "num_experts": 128,
      "num_active_experts": 4,
      "hidden_dim": 2880,
      "num_layers": 36,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 201088,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-2B-Thinking-FP8",
      "total_params_b": 2.0,
      "active_params_b": 2.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 28,
      "num_heads": 16,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "MiMo-V2-Flash",
      "total_params_b": 10.29,
      "active_params_b": 10.29,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 48,
      "num_heads": 64,
      "num_kv_heads": 4,
      "vocab_size": 152576,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Jan-nano",
      "total_params_b": 3.22,
      "active_params_b": 3.22,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2560,
      "num_layers": 36,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.1-8B-Instruct-FP8-Block",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Starling-LM-7B-beta",
      "total_params_b": 7.0,
      "active_params_b": 7.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 32002,
      "max_context_length": 8192,
      "effective_context_length": 8192,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.1-Tulu-3-8B",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128264,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-235B-A22B-FP8",
      "total_params_b": 235.0,
      "active_params_b": 235.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 94,
      "num_heads": 64,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 40960,
      "effective_context_length": 40960,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "cogito-v2-preview-deepseek-671B-MoE-FP8",
      "total_params_b": 671.0,
      "active_params_b": 671.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 128815,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "cogito-v2-preview-llama-70B",
      "total_params_b": 70.0,
      "active_params_b": 70.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Kimi-Dev-72B",
      "total_params_b": 72.0,
      "active_params_b": 72.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Falcon-H1-1.5B-Deep-Instruct",
      "total_params_b": 1.5,
      "active_params_b": 1.5,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 1280,
      "num_layers": 66,
      "num_heads": 6,
      "num_kv_heads": 2,
      "vocab_size": 65537,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "OpenReasoning-Nemotron-32B",
      "total_params_b": 32.0,
      "active_params_b": 32.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 64,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 152064,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen2.5-0.5",
      "total_params_b": 4.0,
      "active_params_b": 4.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 36,
      "num_heads": 16,
      "num_kv_heads": 2,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "gpt-oss-safeguard-120b-BF16",
      "total_params_b": 120.0,
      "active_params_b": 3.75,
      "is_moe": true,
      "num_experts": 128,
      "num_active_experts": 4,
      "hidden_dim": 2880,
      "num_layers": 36,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 201088,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "JanusCoder-14B",
      "total_params_b": 14.0,
      "active_params_b": 14.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 5120,
      "num_layers": 40,
      "num_heads": 40,
      "num_kv_heads": 8,
      "vocab_size": 151936,
      "max_context_length": 32768,
      "effective_context_length": 32768,
      "domains": [
        "general",
        "code"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.1-8B-FP8-Dynamic",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.2-1B-FP8-Block",
      "total_params_b": 1.0,
      "active_params_b": 1.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 16,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.2-1B-FP8-Dynamic",
      "total_params_b": 1.0,
      "active_params_b": 1.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 2048,
      "num_layers": 16,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "cogito-671b-v2.1",
      "total_params_b": 671.0,
      "active_params_b": 671.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 7168,
      "num_layers": 61,
      "num_heads": 128,
      "num_kv_heads": 128,
      "vocab_size": 128815,
      "max_context_length": 163840,
      "effective_context_length": 163840,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.1-8B-FP8-Block",
      "total_params_b": 8.0,
      "active_params_b": 8.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 32,
      "num_heads": 32,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.1-70B-FP8-Block",
      "total_params_b": 70.0,
      "active_params_b": 70.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.1-70B-FP8-Dynamic",
      "total_params_b": 70.0,
      "active_params_b": 70.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 8192,
      "num_layers": 80,
      "num_heads": 64,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Llama-3.2-3B-FP8-Block",
      "total_params_b": 3.0,
      "active_params_b": 3.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 3072,
      "num_layers": 28,
      "num_heads": 24,
      "num_kv_heads": 8,
      "vocab_size": 128256,
      "max_context_length": 131072,
      "effective_context_length": 131072,
      "domains": [
        "general"
      ],
      "capabilities": [],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    },
    {
      "name": "Qwen3-VL-235B-A22B-Instruct-FP8",
      "total_params_b": 235.0,
      "active_params_b": 235.0,
      "is_moe": false,
      "num_experts": null,
      "num_active_experts": null,
      "hidden_dim": 4096,
      "num_layers": 94,
      "num_heads": 64,
      "num_kv_heads": 4,
      "vocab_size": 151936,
      "max_context_length": 262144,
      "effective_context_length": 262144,
      "domains": [
        "general",
        "vision"
      ],
      "capabilities": [
        "vision"
      ],
      "benchmarks": {
        "mmlu": 0,
        "humaneval": 0,
        "gsm8k": 0
      },
      "notes": "Auto-imported from Unsloth"
    }
  ]
}